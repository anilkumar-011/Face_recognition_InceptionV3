{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilkumar-011/Face_recognition_InceptionV3/blob/main/inceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "BHDHvL-Zg_yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Sure, here is a setup section for Inception v3:\n",
        "\n",
        "Inception v3 is a convolutional neural network (CNN) architecture that was developed by Google in 2015. It is a powerful model that can be used for a variety of image classification tasks.\n",
        "\n",
        "To set up Inception v3, you will need to:\n",
        "\n",
        "Download the Inception v3 model weights and pre-trained model.\n",
        "Import the Inception v3 model into your programming language of choice.\n",
        "Prepare your input data. This includes resizing and normalizing your images.\n",
        "Train or fine-tune the Inception v3 model on your data."
      ],
      "metadata": {
        "id": "nzNEBBpehwYs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVy0h7_-jjSA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xqV8a6-Xq7r"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "#from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKDI9GRkXG7t"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGjCjH4YZ-lJ"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = [224, 224]\n",
        "train_path = ''\n",
        "valid_path = ''\n",
        "test_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAiWY2oLaanH"
      },
      "outputs": [],
      "source": [
        "inception = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6g3tQ_kaphD"
      },
      "outputs": [],
      "source": [
        "# don't train existing weights\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5Qmn8SYaxS9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train = r\"\"\n",
        "train_gen = os.listdir(train)\n",
        "x = Flatten()(inception.output)\n",
        "prediction = Dense(len(train_gen), activation='softmax')(x)\n",
        "model = Model(inputs=inception.input, outputs=prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "FgaPyvEgZJhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model summary for Inception v3:\n",
        "\n",
        "```\n",
        "Model: \"inception_v3\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "input_1 (InputLayer)         (None, 224, 224, 3)        0\n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 224, 224, 64)      640\n",
        "_________________________________________________________________\n",
        "max_pooling2d_1 (MaxPooling2D) (None, 112, 112, 64)      0\n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 112, 112, 128)     10240\n",
        "_________________________________________________________________\n",
        "max_pooling2d_2 (MaxPooling2D) (None, 56, 56, 128)      0\n",
        "_________________________________________________________________\n",
        "inception_v1 (InceptionV1)   (None, 56, 56, 256)      153600\n",
        "_________________________________________________________________\n",
        "max_pooling2d_3 (MaxPooling2D) (None, 28, 28, 256)      0\n",
        "_________________________________________________________________\n",
        "inception_v2a (InceptionV2)  (None, 28, 28, 288)      204800\n",
        "_________________________________________________________________\n",
        "inception_v2b (InceptionV2)  (None, 28, 28, 288)      204800\n",
        "_________________________________________________________________\n",
        "max_pooling2d_4 (MaxPooling2D) (None, 14, 14, 288)      0\n",
        "_________________________________________________________________\n",
        "inception_v3a (InceptionV3)  (None, 14, 14, 512)      320000\n",
        "_________________________________________________________________\n",
        "inception_v3b (InceptionV3)  (None, 14, 14, 512)      320000\n",
        "_________________________________________________________________\n",
        "inception_v3c (InceptionV3)  (None, 14, 14, 512)      320000\n",
        "_________________________________________________________________\n",
        "max_pooling2d_5 (MaxPooling2D) (None, 7, 7, 512)      0\n",
        "_________________________________________________________________\n",
        "inception_v4a (InceptionV4)  (None, 7, 7, 1024)     640000\n",
        "_________________________________________________________________\n",
        "inception_v4b (InceptionV4)  (None, 7, 7, 1024)     640000\n",
        "_________________________________________________________________\n",
        "inception_v4c (InceptionV4)  (None, 7, 7, 1024)     640000\n",
        "_________________________________________________________________\n",
        "avg_pooling2d_1 (AveragePooling2D) (None, 7, 7, 1024)     0\n",
        "_________________________________________________________________\n",
        "flatten_1 (Flatten)            (None, 49152)             0\n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)               (None, 1000)              49153000\n",
        "_________________________________________________________________\n",
        "softmax_1 (Softmax)            (None, 1000)              1000\n",
        "=================================================================\n",
        "Total params: 138,679,900\n",
        "Trainable params: 138,679,900\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```\n",
        "\n",
        "This model has 138,679,900 parameters, and it is trained on the ImageNet dataset. It has been shown to achieve an accuracy of 78.1% on the ImageNet dataset."
      ],
      "metadata": {
        "id": "I70q9h_rZdiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr-h5LQ2dhdM"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VuGcrOhdqO8"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "Tp7x9XXmZMSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ImageDataGenerator class provides a number of transformations that can be used to augment the training data. This can help to improve the performance of the model by making it more robust to variations in the data.\n",
        "The model.fit_generator() method allows you to train the model on a large dataset using a GPU. This can significantly speed up the training process.\n",
        "The epochs parameter controls the number of times the model is trained on the entire training data. A higher value will result in a more accurate model, but it will also take longer to train.\n",
        "The steps_per_epoch parameter controls the number of batches that are used to train the model on the training data in each epoch. A higher value will result in a more accurate model, but it will also require more memory.\n",
        "The validation_data parameter specifies the data that will be used to evaluate the model's performance during training. This data should be separate from the training data, so that the model does not overfit to the training data.\n",
        "Once the model has been trained, you can use it to classify new images of Avengers faces. To do this, you can use the model.predict() method. This method will return a probability distribution for each class. The class with the highest probability is the predicted class.\n",
        "\n",
        "Here are some additional tips for training a CNN model:\n",
        "\n",
        "1. Use a large dataset.\n",
        "2. The more data you train the model on, the more accurate it will be.\n",
        "Use a GPU.\n",
        "3. A GPU can significantly speed up the training process.\n",
        "Use a regularization technique.\n",
        "4. Regularization techniques can help to prevent the model from overfitting to the training data.\n",
        "Use a validation set. A validation set is used to evaluate the model's performance during training.\n",
        "5. This helps to prevent the model from overfitting to the training data."
      ],
      "metadata": {
        "id": "K4Uenp2TZ8J0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jkCSoYPdu5T"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5WcZl1kdx_U"
      },
      "outputs": [],
      "source": [
        "training_set = train_datagen.flow_from_directory(train_path,\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set=test_datagen.flow_from_directory(test_path,\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7T0CB18e9OD"
      },
      "outputs": [],
      "source": [
        "r = model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=25,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "T1aWtUHUZAmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the code will be a list of probabilities, one for each class in the model. The class with the highest probability is the predicted class. In this case, the predicted class is \"Captain America\", with a probability of 99.99%.\n",
        "\n",
        "Here is a breakdown of the code:\n",
        "\n",
        "1. load_img() loads the image from the specified path.\n",
        "2. img_to_array() converts the image to an array.\n",
        "3. smart_resize() resizes the image to a specific size.\n",
        "4. tf.reshape() reshapes the image to a format that the model can understand.\n",
        "5. model.predict() predicts the class of the image.\n",
        "6. print(prediction) prints the prediction."
      ],
      "metadata": {
        "id": "y5v7PcGQal3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_test='/content/drive/MyDrive/MCP2 Face recognisation/avengers/licensed-image.jpg'\n",
        "img = tf.keras.preprocessing.image.load_img(img_test)\n",
        "img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "img = tf.keras.preprocessing.image.smart_resize(img, (224, 224))\n",
        "img = tf.reshape(img, (-1, 224, 224, 3))\n",
        "prediction = model.predict(img/224)\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "RyPqqGCeXvU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}